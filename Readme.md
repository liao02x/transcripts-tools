This project is a CLI to sales transcripts utilities with the help of GenAI.

## Project structure:

- `src/` - Source code of the project
- `tests/` - Unit tests of the project
- `i18n/` - Internationalization files for the transcripts and LLM prompts
- `transcripts/` - The transcripts from either the example shared in the question or generated by the project
- `mongodb/` - The MongoDB database volume would go there if you use the docker-compose file
- `docker-compose.yml` - The docker-compose file that can be used to spin up a MongoDB instance
- `.env` - The environment variables file. You can use it to set the environment variables for the project.

## Project overview:

The project is a CLI tool that can be used to generate transcripts, summarize a transcript, and answer questions based on transcripts. Each of these functionalities is implemented in a separate file in the `src/commands` directory. The project uses the OpenAI API to perform these tasks. The project is structured in a way that it can be easily extended to add more functionalities. The chats with the LLM can be saved in a MongoDB database if the `MONGODB_URI` is provided in the `.env` file.

#### Commands overview:

The commands are built using the `commander` package, with the support of `ora` package to show the loading spinner while the command is waiting for the LLM to respond.
`tsx` is used to execute the commands and `mocha` is used to run the tests.
The commands are built in a way that you can choose to either having the necessary arguments in the command line when you start it or you can provide them later with a interactive prompt.

#### I18n support:

The project added support for the transcripts and questions in different languages. To be able to use this feature, you need to choose a model that has multilingual support. You can set the language in the command line with "-l" or "--lang" flag. The default language is English. The supported languages are English(en), Spanish(es), and French(fr). The supported foreign language is only used for the transcripts and the LLM chatting/response. The system messages are always in English.

## Before running the project:

Before running the project, you need to install the dependencies. You can do that by running the following command:

```bash
npm install
```

You'll also need to set up the environment variables in the `.env` file.
`OPENAI_BASE_URL`, `OPENAI_API_KEY` and `OPENAI_MODEL` are required, but you can also use a OpenAI compatible API like ollama.
`MONGODB_URI` is optional, if it's provided, at the end of the ask command, the question and the answer will be saved in the database.
`TRANSCRIPT_OUTPUT_PATH` is optional and defaults to `transcripts/`, it's the path where the generated transcripts will be saved.
If you want to quickly spin up a MongoDB instance, you can use the `docker-compose.yml` file to do that. You can run the following command to start the MongoDB instance:

```bash
docker-compose up -d
```

## How to run the project:

For the three parts of the project, each of them has its own subcommand. If you run the main command with

```bash
npm run start
```

It will show you the help message with the available subcommands. The help message is also available with the `--help` flag for both the main command and the subcommands.

---

#### Generating a transcript:

For the generation of the transcripts, you can run the following command:

```bash
npm run start generate
```

or simply

```bash
npm run generate
# Usage: transcripts-tools generate [options]
#
# Generate a sales call transcript
#
# Options:
#   -f, --file <string>  save transcript to the file (default: "")
#   -l, --lang <string>  language in use (choices: "en", "es", "fr", default: "en")
#   -h, --help           display help for command
```

This would start the generation of the transcript in the language specified. While it's being generated, you'll see a preview of it in the console. The final transcript will be post processed to clean it up. If you specified the file name in the command line like this:

```
npm run generate -- -f transcript1.txt
```

The transcript will be saved in the `transcripts/` directory with the name `transcript1.txt`. If you didn't specify the file name, the command will ask you if you want to save it or not.

---

#### Summarizing a transcript:

For the summarization of the transcripts, you can run the following command:

```bash
npm run summarize
# Usage: transcripts-tools summarize [options]
#
# Summarize a sales call transcript
#
# Options:
#   -f, --file <string>  read transcript from the file (default: "")
#   -l, --lang <string>  language in use (choices: "en", "es", "fr", default: "en")
#   -h, --help           display help for command
```

Same as the generate command, you can choose to specify the path to the transcript file from the command line or there will be an interactive prompt to ask you for it. The summary will be shown in the console in the language specified.

---

#### Asking questions:

For asking questions based on the transcripts, you can run the following command:

```bash
npm run ask
# Usage: transcripts-tools ask [options]
#
# Ask questions about a sales call transcript
#
# Options:
#   -f, --file <string>      read transcript from the file (default: "")
#   -q, --question <string>  question about the transcript (default: "")
#   -l, --lang <string>      language in use (choices: "en", "es", "fr", default: "en")
#   -h, --help               display help for command
```

You can specify the path to the transcript file and the question from the command line or there will be an interactive prompt to ask you for them. If you provided a initial question in the command line, it'll be used to ask the LLM. The answer will be shown in the console. After each question, there would be a prompt to ask if you want to ask another question or not. If no more questions are entered, the command would try to save the questions and answers in the database if the `MONGODB_URI` is provided and then exit.

## Thought process:

When I first read the questions, I decided to start with the generation as it's the base for the other two functionalities. I started to build a quick wrapper around the OpenAI API with their sdk to generate something and output it in the console.
I used ollama from my local computer to save some credits. I picked model mistral-nemo(https://mistral.ai/news/mistral-nemo/) since it has multilingual support and it's about the size that I can run from my local computer. I was using `openai.chat.completions.create` to generate the result, later I found `openai.beta.chat.completions.stream` that supported both streaming the result and returning the whole response when it's done, so that the user would be able to figure out if the result is good enough to let it continue or not early on. I figured the LLM might need a complete conversation to generate a better result, so I used a GenAI to complete the example transcript provided in the question.
That's when I started to notice that there is ~10 seconds delay before the api started to streaming. To make the user experience better, I added a loading spinner with `ora` package and later used in a lot of places in the project. The `ora` package brought some issues with the execcutor `ts-node` so I switched to `tsx`. After a few tests on the generating results, I added the post process to clean up the generated result to make it more like a transcript.
After the generation was done, I worked on the util functions that read and write the files given the path that the user provided in the prompt. I figured a way to handle the flow so that the code can be simplified. I used the same async control flow for both reading and writing the files.
Then I focused on the summarization part. I modified the system prompt a bit to ask the LLM to summarize the transcript. This one is pretty straightforward and I just need to send the response to the console.
The last part was the asking questions. I researched on how to keep the conversation context so that the user can ask follow up questions. I found an example on OpenAI's forum and it's pretty straightforward. Save the assistant response, compose it with the previous conversation and the new question, and send it as message to the LLM. I guess that's also saving the time on the task where I need to persist the user's chat with the assistant. Since I already have the whole conversation, all I need to do is to just save it to the database.
After I finished the main functionalities, I started to add the CLI commands with `commander` package. I moved the command files to a separate directory and added the subcommands for them. To support the command line arguments, I modified a couple of the implementation so that the read/write transcript function would be able to start with a initial value without asking in the prompt.
Later I was planning on the i18n support. Initially I was thinking about supporting the system messages as well, but I figured there are prompts that ask the user to enter "yes" or "no" and it would be tricky to support that in multiple languages(especially when I don't know either French or Spanish). Then I decided to only support the transcripts and the LLM chatting. The language selected is maintained in the config so I don't need to pass it around in the functions. I implemented a simple i18n support with the text keys and the translation(by GenAI) in the `i18n` directory.
Lastly, I added some unit tests for some parts of the project. I added the db access functions to support persisting the chat and the docker compose file that I found here(https://hub.docker.com/_/mongo) to support a quick test on the db access functionality.

## Results:
Sharing a few results from the commands:

#### Generate transcript in French:
```bash
npm run generate -- -l fr
```
```
> transcripts-tools@1.0.0 generate
> tsx src generate -l fr

[ASSISTANT] Below is the preview of the transcript:

00:00:00 Alex (google.ai) : Bonjour Aisha.
00:00:02 Aisha (ibm.watson) : Salut Alex, comment allez-vous ?
00:00:05 Alex (google.ai) : Très bien, merci. Nous avons besoin de 8000 TPU supplémentaires pour nos derniers modèles d'apprentissage en profondeur. Êtes-vous en mesure d'honorer cette demande ?
00:00:06 Aisha (ibm.watson) : Nous comprenons l'importance des TPU dans votre travail, Alex, mais actuellement nous ne pouvons pas fournir le nombre exact de 8000 que vous requérez. Cependant, si vous êtes flexible avec ce chiffre, nous pourrions trouver une solution plus réaliste comme 4500.
00:00:10 Alex (google.ai) : Je comprends votre position, Aisha, mais nos modèles ont besoin de toute la puissance de calcul disponible pour atteindre leur plein potentiel. Serait-il possible d'augmenter ce chiffre d'ici quelques mois ? Nous avons des délais serrés à respecter.
00:00:20 Aisha (ibm.watson) : Je compatis avec votre situation. Malheureusement, l'allocation de ressources est actuellement fixée à un niveau inférieur. Si vous êtes ouvert à une solution progressive, nous pourrions augmenter le nombre de TPU à mesure que nous avons des disponibilités supplémentaires. Cependant, au départ, je peux vous offrir 4500 TPU.
00:00:35 Alex (google.ai) : Cela devient un peu problématique pour nous, Aisha. Nous pourrions peut-être rencontrer à mi-chemin avec quelque chose comme 6000 TPU ? Cela nous donnerait plus de puissance de calcul pour les mois à venir et nous aiderait à respecter nos délais.
00:00:50 Aisha (ibm.watson) : J'apprécie votre souplesse, Alex. Je vais discuter avec notre équipe technique et voir si une augmentation progressive du nombre de TPU peut être possible dans un avenir proche. Nous allons vous contacter pour discuter des détails plus approfondis.
00:01:00 Alex (google.ai) : Cela me semble faisable, Aisha. Merci beaucoup pour votre temps et votre considération. Nous attendons votre appel.
00:01:10 Aisha (ibm.watson) : De rien, Alex. Nous allons trouver une solution qui convienne à tous les deux. Talk soon.
00:01:15 Alex (google.ai) : À bientôt, Aisha.


[ASSISTANT] Above is the generated transcript.
[SYSTEM] Do you want to save the transcript? Type 'no' to skip:
```

#### Summarize the transcript:
```bash
npm run summarize -- -l es -f transcripts/testes1.txt
```
```
> transcripts-tools@1.0.0 summarize
> tsx src summarize -l es -f transcripts/testes1.txt

[ASSISTANT] Below is the summary of the transcript:

El resumen de la conversación es el siguiente:

Comenzó con Sam (de openai.com) llamando a Maria (de ibm.com) para hablar sobre su acuerdo de procesamiento cognitivo. Sam quería aumentar su capacidad en un 15% debido al crecimiento de su empresa. Sin embargo, Maria respondió que actualmente estaban ajustados en cuanto a capacidad, y solo podían ofrecer un aumento del 10%. Sam pidió considerar un aumento mayor, como el 12% o el 15%, pero eso no fue posible según Maria.

Después de negociar, llegaron a un acuerdo: comenzarán con un aumento del 10% durante seis meses, luego reevaluarán las capacidades para Betrachtel ein mögliches erhöhtes zur Verfügung stehen. Sam aceptó agradeciendo la comprensión de Maria.
```

## Transparency on AI Usage:

The GenAI is used in generating the LLM prompts and transcript examples. More details in `src/i18n/en.ts` and other files in the `src/i18n/` directory.
This document is written with the help of GenAI. Other than that, the project doesn't use any GenAI or similar services on the creating the code.
